{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takedatmh/toyama/blob/main/2_9_PreTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3K2wy9IIrwpr",
        "outputId": "f94ed9eb-2e9f-4506-8b13-cc5d3e15a451"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m399.4/491.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers datasets accelerate sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3xmwR5GtSzX",
        "outputId": "ad9479a4-62c2-40a6-9768-99dd32fb118f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/ColabNotebooks/Toyama_University/PreTrain\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775,
          "referenced_widgets": [
            "72b798e4b6ac4e9889027cb719ba8c0f",
            "dc070fcf09b54b8c86f14bcdbc746350",
            "ec80d5f047e648afb90aef564bb93e15",
            "695e258e43cb4d45800f5c36aa603bdd",
            "53d57ab5da0b4ca68e0a8f59e5facef8",
            "ec63ad3b6daa48c2a4fdc7c585014d9e",
            "5f23a3422041498f949ccf743115f9ca",
            "1fa2df8ac0fd452f8a00b6e064af0ea7",
            "e6b9a4a85cd24669b25ae5c1e4c0fbad",
            "2caaa2ba1ac041eb86ce989d46549338",
            "7f8c91362f5c4ad1ae2518cb9ab9f9e3",
            "c6d1e266c8c447e89c0485da4ffcff47",
            "c3081314a0bf4330897035761e80f210",
            "7ac3b62c2311463eae21770fef8b02ff",
            "6d35bf9d86c04e95ba6d07f8c82c10bb",
            "70da608380d641fc9141f31de52ce854",
            "a6e676b795184c92b23dfbc1eb0c694e",
            "52b787dcf6704f7895b8cb1dbbc9a387",
            "6a3521f7827247b99f782dbc023fcd7a",
            "d77bdaa7b96b4fd2b6064802c1831d85",
            "f6cbd2b93d8e41e2a7b9a7d7fcaabde6",
            "23ceb583edc840bc8ca659f8bb278fd2",
            "b6994c6fec54468ca696d2865a7f9c50",
            "4facb6cb895144c4b38537439069b32a",
            "73d5f62876424799ab919cbd0f92aa4a",
            "8648d2a41f4246eb9b50ccdf8706f3c5",
            "a177a85d1ec2436baae2c71d8c6106fe",
            "7fadfee9efd44ecf9deda854342a2b76",
            "6b29f49deccc4cf9bbe961c90c5ea864",
            "1c0af4007e754fbfb4f52a1978628b61",
            "c9befe04881c4f91b61b870e9e20f8ec",
            "06065b5595944fa784d29a25211f93d0",
            "4d320f8c3b2b44bb8e473db19535815e",
            "2e600b0aa1c148d78f57379c98cb9dd3",
            "882ea203a80f4d19a0b712fd4e959731",
            "e724fe70a5ca40339a71330999dc82ca",
            "b23a02835324499383ae2ccb2edbfbcf",
            "bded725d3e4a45068dd04952b91daf3d",
            "713cae94d2a546d19838557dea1d1e52",
            "3017fb1beae246c290adbb1202a9059f",
            "75c80da658d547799873cb3d2357131b",
            "9d92cd6eb27e430f884acded5d7c984d",
            "ab32f9344dfe402ea9ae07134acc2ff7",
            "b1a6b19b951e4fb4981e9941659b688b",
            "61017da84ea0477b8572808ce36da08e",
            "807e306669fb40ec9afcd7276fdaac68",
            "6707925b41304fa587bd9875dac24631",
            "46e52456ff174b288a51c7c42f2b794e",
            "0e7f8003cc0a41d5b06f5ea93b2af630",
            "c2fbdd50cba346cf934e3eb5647d57ac",
            "e55b4cdd47394ccb9339ddd958f98347",
            "e7eff5442b4846698fdf86d7e7809f19",
            "c4c690e0bff34d33a2efff8deb990a55",
            "12a11c90a4eb493ab338b26fe17cb407",
            "5ccba4c0012e4ef1add67c7f9d53593e",
            "229e8a154c0c4bfdbcc469e52e3540c6",
            "eb2484b124b04d4c8b870202a67b4808",
            "f944946a01b440818977e7189119eb61",
            "c5b74ee502aa4182845494196f9e04d1",
            "84a3dc350e1249cc813ce7837d7a16a1",
            "daf307137e0e43b590548d63f4c72299",
            "2cd761c2b51a43b2866db389ff597c3d",
            "643dd80412f442fd8d1a35cb69877a10",
            "8fc99ea8b60043559656a3382f741040",
            "b43e17eb484448a08930a2f2f306a7e4",
            "d16f4259e2d0436d8c8d66df948d92a5"
          ]
        },
        "id": "Bi0DeTLFr7bY",
        "outputId": "54c544d5-d756-4335-ee01-c0b619822ac5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72b798e4b6ac4e9889027cb719ba8c0f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/282 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6d1e266c8c447e89c0485da4ffcff47",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "spiece.model:   0%|          | 0.00/806k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6994c6fec54468ca696d2865a7f9c50",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/153 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e600b0aa1c148d78f57379c98cb9dd3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61017da84ea0477b8572808ce36da08e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/454M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "229e8a154c0c4bfdbcc469e52e3540c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:26, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>5.813800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.122400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.338000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>6.175800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>4.405600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.011600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.850900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.804800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.422900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.446100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10, training_loss=4.039201068878174, metrics={'train_runtime': 27.5584, 'train_samples_per_second': 0.544, 'train_steps_per_second': 0.363, 'total_flos': 979845120000.0, 'train_loss': 4.039201068878174, 'epoch': 5.0})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    Trainer, TrainingArguments,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# 1. モデルとトークナイザのロード\n",
        "model_name = \"rinna/japanese-gpt2-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# 2. 継続学習させる日本語データ（例：テスト用の短文）\n",
        "texts = [\n",
        "    \"日本の首都は東京です。\",\n",
        "    \"富士山は日本で最も高い山です。\",\n",
        "    \"りんなは日本語に特化した大規模言語モデルです。\"\n",
        "]\n",
        "\n",
        "dataset = Dataset.from_dict({\"text\": texts})\n",
        "\n",
        "# 3. トークン化\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# 4. データコラレータ（Language Modeling用）\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # GPT系はMaskedではなくCausal\n",
        ")\n",
        "\n",
        "# 5. 学習設定\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./rinna-continued\",\n",
        "    per_device_train_batch_size=2,\n",
        "    num_train_epochs=5,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=1,\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# 6. トレーナー構築と学習開始\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5Qro3HLtRQH",
        "outputId": "07a62db9-bc3d-4dd6-fa38-72363cdccb41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/ColabNotebooks/Toyama_University/PreTrain/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/ColabNotebooks/Toyama_University/PreTrain/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/ColabNotebooks/Toyama_University/PreTrain/spiece.model',\n",
              " '/content/drive/MyDrive/ColabNotebooks/Toyama_University/PreTrain/added_tokens.json',\n",
              " '/content/drive/MyDrive/ColabNotebooks/Toyama_University/PreTrain/tokenizer.json')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# モデルとトークナイザを保存（Google Drive）\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOUFT9Z_uRbD",
        "outputId": "9ebdaa45-13e3-4fbc-9491-deff84214b46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧠 モデルの回答: りんなは、どのような言語モデルですか?なぜ私たちの学校では、プログラミングやデザインといった「学び」をどのように学んでいますか? it(情報技術)教育はどうやって生まれていますか? なぜプログラミングやデザインは生まれたのでしょうか。 「学び」とはどんなものでしょうか? 授業の中で教えて下さった生徒さんに伝えたいことはありますか? 私自身が学んだ知識だけでは十分ですが、お客様から得た情報をより効果的に活用するために実践しているスキルなどを教えていただけますか\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# 保存済みモデルとトークナイザをロード\n",
        "model = AutoModelForCausalLM.from_pretrained(save_path).to(\"cuda\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
        "\n",
        "# 推論用プロンプト\n",
        "prompt = \"りんなは、どのような言語モデルですか？\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# 推論設定\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=0.8,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "# 結果表示\n",
        "generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"🧠 モデルの回答:\", generated.replace(prompt, \"\").strip())\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMAY3xoZ/fCzMFpWM1367/f",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}