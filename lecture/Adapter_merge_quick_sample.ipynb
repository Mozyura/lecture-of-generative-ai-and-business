{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO0YUFfYg2y/l4rY//bkeef",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takedatmh/toyama/blob/main/Adapter_merge_quick_sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx5yf1vXkVaA",
        "outputId": "ada56e53-b08e-4f88-8cfe-1447cd076cb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRAマージ - 即座に実行可能なコード\n",
            "==================================================\n",
            "\n",
            "### 例1: OPT-125Mでの基本マージ（CPU可、メモリ500MB程度）\n",
            "モデルをロード中...\n",
            "LoRAパラメータ数: 147,456\n",
            "\n",
            "アダプターをマージ中...\n",
            "\n",
            "プロンプト: 'AI technology is'\n",
            "生成中...\n",
            "結果: AI technology is a \"thought-driven\" process. We live with artificial intelligence, and we live with it. Today, the tech giant IBM\n",
            "\n",
            "\n",
            "### 例2: GPT-2でのマージ例（CPU可、メモリ1.5GB程度）\n",
            "GPT-2をロード中...\n",
            "\n",
            "戦略: tech_creative_mix\n",
            "  入力: The future of artificial intelligence\n",
            "  出力: The future of artificial intelligence is unclear, but the company is set to unveil a new platform to take advantage of the high-tech capabilities of the next generation of AI.\n",
            "\n",
            "The new platform,\n",
            "\n",
            "戦略: all_balanced\n",
            "  入力: The future of artificial intelligence\n",
            "  出力: The future of artificial intelligence and the future of the world is in flux.\n",
            "\n",
            "This post originally appeared on Digital Trends.\n",
            "\n",
            "We are excited to announce that Samsung has finally created a new smart\n",
            "\n",
            "\n",
            "### 例3: マージ手法の比較\n",
            "\n",
            "テストプロンプト: 'Machine learning is'\n",
            "\n",
            "各マージ手法の結果:\n",
            "\n",
            "[linear]: Machine learning is the most efficient way to learn, and it's going to do a lot of good for you.\n",
            "\n",
            "\n",
            "[cat]: Machine learning is a fundamental part of the human mind. It is often referred to as machine learning.\n",
            "\n",
            "The human\n",
            "\n",
            "[ties]: Machine learning is the next big thing but it’s going to take a while to get here. It’\n",
            "\n",
            "[dare_linear]: Machine learning is a new approach to solving complex problems, from the engineering perspective, and as such is expected to be a\n",
            "\n",
            "\n",
            "### 例4: 実用的なヘルパー関数の使用\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ヘルパー関数の結果:\n",
            "The secret to happiness is to be happy.\n",
            "I'm not sure if you're being sarcastic or not, but I think you're right.\n",
            "\n",
            "==================================================\n",
            "✅ すべての例が完了しました！\n",
            "\n",
            "実行環境:\n",
            "- PyTorch バージョン: 2.6.0+cu124\n",
            "- デバイス: CUDA\n",
            "- メモリ効率: すべての例がCPUで実行可能\n",
            "\n",
            "💡 ヒント:\n",
            "1. より大きなモデルを試す場合は、load_in_8bit=True を使用\n",
            "2. 実際の訓練済みアダプターはHuggingFace Hubで検索可能\n",
            "3. マージ手法を変えることで異なる特性が得られます\n",
            "   - linear: バランスの取れた結合\n",
            "   - cat: 完全な機能保持（メモリ使用増）\n",
            "   - ties: スパース性を考慮した結合\n",
            "   - dare_linear: ランダム性を加えた結合\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "LoRAマージ - 今すぐ実行できる最小構成コード\n",
        "CPUでも動作し、実際のHuggingFaceアダプターを使用可能\n",
        "\"\"\"\n",
        "\n",
        "# 必要なパッケージのインストール（初回のみ）\n",
        "# pip install transformers peft torch --upgrade\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel, LoraConfig, get_peft_model, TaskType\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"LoRAマージ - 即座に実行可能なコード\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# ===========================\n",
        "# 実行例1: 最軽量モデルで基本的なマージ\n",
        "# ===========================\n",
        "\n",
        "print(\"\\n### 例1: OPT-125Mでの基本マージ（CPU可、メモリ500MB程度）\")\n",
        "\n",
        "# モデルのロード\n",
        "print(\"モデルをロード中...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
        "\n",
        "# LoRAアダプター設定\n",
        "peft_config = LoraConfig(\n",
        "    r=4,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "# PEFTモデル作成\n",
        "model = get_peft_model(base_model, peft_config)\n",
        "print(f\"LoRAパラメータ数: {model.get_nb_trainable_parameters()[0]:,}\")\n",
        "\n",
        "# 2つ目のアダプターを追加\n",
        "model.add_adapter(\"adapter2\", peft_config)\n",
        "\n",
        "# アダプターのマージ実行\n",
        "print(\"\\nアダプターをマージ中...\")\n",
        "model.add_weighted_adapter(\n",
        "    adapters=[\"default\", \"adapter2\"],\n",
        "    weights=[0.6, 0.4],\n",
        "    adapter_name=\"merged\",\n",
        "    combination_type=\"linear\"\n",
        ")\n",
        "\n",
        "# マージしたアダプターで生成\n",
        "model.set_adapter(\"merged\")\n",
        "prompt = \"AI technology is\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "print(f\"\\nプロンプト: '{prompt}'\")\n",
        "print(\"生成中...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(**inputs, max_length=30, temperature=0.8, do_sample=True)\n",
        "\n",
        "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"結果: {result}\")\n",
        "\n",
        "# ===========================\n",
        "# 実行例2: GPT-2での実用的な例\n",
        "# ===========================\n",
        "\n",
        "print(\"\\n\\n### 例2: GPT-2でのマージ例（CPU可、メモリ1.5GB程度）\")\n",
        "\n",
        "# GPT-2モデル（日本語でも英語でも動作）\n",
        "print(\"GPT-2をロード中...\")\n",
        "gpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
        "\n",
        "# GPT-2用のLoRA設定\n",
        "gpt2_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],  # GPT-2のアーキテクチャに対応\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "# PEFT model creation with the first adapter (\"default\")\n",
        "gpt2_peft = get_peft_model(gpt2_model, gpt2_config)\n",
        "\n",
        "# Add additional adapters\n",
        "adapters = {\n",
        "    \"technical\": {\"r\": 8, \"alpha\": 32},\n",
        "    \"creative\": {\"r\": 8, \"alpha\": 16},\n",
        "    \"balanced\": {\"r\": 8, \"alpha\": 24}\n",
        "}\n",
        "\n",
        "for name, params in adapters.items():\n",
        "    config = LoraConfig(\n",
        "        r=params[\"r\"],\n",
        "        lora_alpha=params[\"alpha\"],\n",
        "        target_modules=[\"c_attn\", \"c_proj\"],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "    )\n",
        "    # Add adapter with the specified name\n",
        "    gpt2_peft.add_adapter(name, config)\n",
        "\n",
        "\n",
        "# 様々なマージ戦略\n",
        "merge_strategies = [\n",
        "    {\n",
        "        \"name\": \"tech_creative_mix\",\n",
        "        \"adapters\": [\"default\", \"technical\", \"creative\"],\n",
        "        \"weights\": [0.2, 0.5, 0.3],\n",
        "        \"method\": \"linear\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"all_balanced\",\n",
        "        \"adapters\": [\"default\", \"technical\", \"creative\", \"balanced\"],\n",
        "        \"weights\": [0.25, 0.25, 0.25, 0.25],\n",
        "        \"method\": \"linear\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# マージ実行とテスト\n",
        "test_prompts = [\n",
        "    \"The future of artificial intelligence\",\n",
        "    \"How to write better code\",\n",
        "    \"Once upon a time in a digital world\"\n",
        "]\n",
        "\n",
        "for strategy in merge_strategies:\n",
        "    print(f\"\\n戦略: {strategy['name']}\")\n",
        "\n",
        "    # マージ実行\n",
        "    gpt2_peft.add_weighted_adapter(\n",
        "        adapters=strategy[\"adapters\"],\n",
        "        weights=strategy[\"weights\"],\n",
        "        adapter_name=strategy[\"name\"],\n",
        "        combination_type=strategy[\"method\"]\n",
        "    )\n",
        "\n",
        "    gpt2_peft.set_adapter(strategy[\"name\"])\n",
        "\n",
        "    # 最初のプロンプトでテスト\n",
        "    prompt = test_prompts[0]\n",
        "    inputs = gpt2_tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = gpt2_peft.generate(\n",
        "            **inputs,\n",
        "            max_length=40,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=gpt2_tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    result = gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"  入力: {prompt}\")\n",
        "    print(f\"  出力: {result}\")\n",
        "\n",
        "# ===========================\n",
        "# 実行例3: 異なるマージ手法の比較\n",
        "# ===========================\n",
        "\n",
        "print(\"\\n\\n### 例3: マージ手法の比較\")\n",
        "\n",
        "# 新しいベースモデル\n",
        "comparison_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")\n",
        "comparison_peft = get_peft_model(comparison_model, peft_config)\n",
        "\n",
        "# アダプターを追加\n",
        "for i in range(2):\n",
        "    comparison_peft.add_adapter(f\"adapter_{i+1}\", peft_config)\n",
        "\n",
        "# 異なるマージ手法を試す\n",
        "methods_to_test = {\n",
        "    \"linear\": {},\n",
        "    \"cat\": {},  # concatenation\n",
        "    \"ties\": {\"density\": 0.5, \"majority_sign_method\": \"frequency\"},\n",
        "    \"dare_linear\": {\"density\": 0.8}\n",
        "}\n",
        "\n",
        "prompt = \"Machine learning is\"\n",
        "print(f\"\\nテストプロンプト: '{prompt}'\")\n",
        "print(\"\\n各マージ手法の結果:\")\n",
        "\n",
        "for method_name, extra_params in methods_to_test.items():\n",
        "    try:\n",
        "        # マージ実行\n",
        "        merge_name = f\"merged_{method_name}\"\n",
        "        comparison_peft.add_weighted_adapter(\n",
        "            adapters=[\"default\", \"adapter_1\"],\n",
        "            weights=[0.6, 0.4],\n",
        "            adapter_name=merge_name,\n",
        "            combination_type=method_name,\n",
        "            **extra_params\n",
        "        )\n",
        "\n",
        "        comparison_peft.set_adapter(merge_name)\n",
        "\n",
        "        # 生成\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            outputs = comparison_peft.generate(\n",
        "                **inputs,\n",
        "                max_length=25,\n",
        "                temperature=0.7,\n",
        "                do_sample=True\n",
        "            )\n",
        "\n",
        "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        print(f\"\\n[{method_name}]: {result}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n[{method_name}]: エラー - {str(e)[:50]}...\")\n",
        "\n",
        "# ===========================\n",
        "# 実行例4: 実用的なヘルパー関数\n",
        "# ===========================\n",
        "\n",
        "def quick_lora_merge(\n",
        "    model_name=\"facebook/opt-125m\",\n",
        "    adapter_configs=None,\n",
        "    merge_weights=None,\n",
        "    merge_method=\"linear\",\n",
        "    test_prompt=\"Hello, AI!\"\n",
        "):\n",
        "    \"\"\"\n",
        "    LoRAマージを簡単に実行するヘルパー関数\n",
        "\n",
        "    Args:\n",
        "        model_name: ベースモデル名\n",
        "        adapter_configs: アダプター設定のリスト\n",
        "        merge_weights: マージ時の重み\n",
        "        merge_method: マージ手法\n",
        "        test_prompt: テスト用プロンプト\n",
        "\n",
        "    Returns:\n",
        "        生成されたテキスト\n",
        "    \"\"\"\n",
        "    # デフォルト設定\n",
        "    if adapter_configs is None:\n",
        "        adapter_configs = [\n",
        "            {\"r\": 4, \"alpha\": 16},\n",
        "            {\"r\": 4, \"alpha\": 32}\n",
        "        ]\n",
        "\n",
        "    if merge_weights is None:\n",
        "        merge_weights = [0.5] * len(adapter_configs)\n",
        "\n",
        "    # モデルロード\n",
        "    base = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    tok = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # 最初のアダプター\n",
        "    config = LoraConfig(\n",
        "        r=adapter_configs[0][\"r\"],\n",
        "        lora_alpha=adapter_configs[0][\"alpha\"],\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "    )\n",
        "\n",
        "    peft_model = get_peft_model(base, config)\n",
        "\n",
        "    # 追加アダプター\n",
        "    adapter_names = [\"default\"]\n",
        "    for i, cfg in enumerate(adapter_configs[1:], 1):\n",
        "        name = f\"adapter_{i}\"\n",
        "        adapter_names.append(name)\n",
        "        config = LoraConfig(\n",
        "            r=cfg[\"r\"],\n",
        "            lora_alpha=cfg[\"alpha\"],\n",
        "            target_modules=[\"q_proj\", \"v_proj\"],\n",
        "            lora_dropout=0.1,\n",
        "            bias=\"none\",\n",
        "            task_type=TaskType.CAUSAL_LM,\n",
        "        )\n",
        "        peft_model.add_adapter(name, config)\n",
        "\n",
        "    # マージ\n",
        "    peft_model.add_weighted_adapter(\n",
        "        adapters=adapter_names,\n",
        "        weights=merge_weights,\n",
        "        adapter_name=\"final_merge\",\n",
        "        combination_type=merge_method\n",
        "    )\n",
        "\n",
        "    peft_model.set_adapter(\"final_merge\")\n",
        "\n",
        "    # 生成\n",
        "    inputs = tok(test_prompt, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = peft_model.generate(**inputs, max_length=50, temperature=0.8)\n",
        "\n",
        "    return tok.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# ヘルパー関数の使用例\n",
        "print(\"\\n\\n### 例4: 実用的なヘルパー関数の使用\")\n",
        "\n",
        "result = quick_lora_merge(\n",
        "    adapter_configs=[\n",
        "        {\"r\": 8, \"alpha\": 16},\n",
        "        {\"r\": 8, \"alpha\": 32},\n",
        "        {\"r\": 8, \"alpha\": 24}\n",
        "    ],\n",
        "    merge_weights=[0.5, 0.3, 0.2],\n",
        "    merge_method=\"linear\",\n",
        "    test_prompt=\"The secret to happiness is\"\n",
        ")\n",
        "\n",
        "print(f\"ヘルパー関数の結果:\\n{result}\")\n",
        "\n",
        "# ===========================\n",
        "# 実行完了\n",
        "# ===========================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"すべての例が完了しました！\")\n",
        "print(\"\\n実行環境:\")\n",
        "print(f\"- PyTorch バージョン: {torch.__version__}\")\n",
        "print(f\"- デバイス: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
        "print(f\"- メモリ効率: すべての例がCPUで実行可能\")\n",
        "\n",
        "print(\"\\n💡 ヒント:\")\n",
        "print(\"1. より大きなモデルを試す場合は、load_in_8bit=True を使用\")\n",
        "print(\"2. 実際の訓練済みアダプターはHuggingFace Hubで検索可能\")\n",
        "print(\"3. マージ手法を変えることで異なる特性が得られます\")\n",
        "print(\"   - linear: バランスの取れた結合\")\n",
        "print(\"   - cat: 完全な機能保持（メモリ使用増）\")\n",
        "print(\"   - ties: スパース性を考慮した結合\")\n",
        "print(\"   - dare_linear: ランダム性を加えた結合\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "このコードは、**LoRA（Low-Rank Adaptation）アダプターのマージ技術**を実演するもので、複数のLoRAアダプターを組み合わせて新しいモデルの振る舞いを作り出す方法を示しています。\n",
        "\n",
        "## コードの主要な構成\n",
        "\n",
        "### 1. **LoRAアダプターの基本設定パラメータ**\n",
        "\n",
        "```python\n",
        "peft_config = LoraConfig(\n",
        "    r=4,                              # ランク（低ランク分解の次元数）\n",
        "    lora_alpha=16,                    # スケーリングファクター\n",
        "    target_modules=[\"q_proj\", \"v_proj\"], # 適用対象のモジュール\n",
        "    lora_dropout=0.1,                 # ドロップアウト率\n",
        "    bias=\"none\",                      # バイアスの扱い\n",
        "    task_type=TaskType.CAUSAL_LM,    # タスクタイプ\n",
        ")\n",
        "```\n",
        "\n",
        "### 2. **各パラメータの詳細解説**\n",
        "\n",
        "#### **`r`（ランク）**\n",
        "- 低ランク行列分解の次元数を指定\n",
        "- 小さい値（4-8）：メモリ効率的だが表現力は限定的\n",
        "- 大きい値（16-32）：より豊かな表現が可能だがメモリ使用量増加\n",
        "- 例：`r=4`は元の重み行列を4次元の低ランク行列で近似\n",
        "\n",
        "#### **`lora_alpha`（アルファ）**\n",
        "- LoRAの更新をスケールする係数\n",
        "- 実質的な学習率のような役割：`lora_alpha / r`\n",
        "- 例：`r=4, lora_alpha=16`の場合、実効的なスケーリングは4倍\n",
        "\n",
        "#### **`target_modules`（対象モジュール）**\n",
        "- LoRAを適用する具体的なレイヤー/モジュール\n",
        "- モデルアーキテクチャごとに異なる：\n",
        "  - OPT/LLaMA: `[\"q_proj\", \"v_proj\"]`（Query/Value投影）\n",
        "  - GPT-2: `[\"c_attn\", \"c_proj\"]`（Attention/Projection）\n",
        "- これらは自己注意機構の重要な部分\n",
        "\n",
        "### 3. **マージ戦略の実装例**\n",
        "\n",
        "```python\n",
        "# 複数アダプターの重み付きマージ\n",
        "model.add_weighted_adapter(\n",
        "    adapters=[\"default\", \"adapter2\"],  # マージするアダプター名\n",
        "    weights=[0.6, 0.4],               # 各アダプターの重み\n",
        "    adapter_name=\"merged\",            # 新しいアダプター名\n",
        "    combination_type=\"linear\"         # マージ手法\n",
        ")\n",
        "```\n",
        "\n",
        "### 4. **異なるマージ手法**\n",
        "\n",
        "コードでは4つのマージ手法を比較：\n",
        "\n",
        "- **`linear`**: 単純な線形結合（重み付き平均）\n",
        "- **`cat`**: 連結（全機能保持、メモリ増）\n",
        "- **`ties`**: スパース性考慮（密度パラメータ付き）\n",
        "- **`dare_linear`**: ランダム性を加えた線形結合\n",
        "\n",
        "### 5. **実用的な使用例**\n",
        "\n",
        "```python\n",
        "# GPT-2での複数アダプター設定\n",
        "adapters = {\n",
        "    \"technical\": {\"r\": 8, \"alpha\": 32},  # 技術的な文章用\n",
        "    \"creative\": {\"r\": 8, \"alpha\": 16},   # 創造的な文章用\n",
        "    \"balanced\": {\"r\": 8, \"alpha\": 24}    # バランス型\n",
        "}\n",
        "```\n",
        "\n",
        "各アダプターは異なる`alpha`値を持ち、異なる「性格」を表現：\n",
        "- **technical**: 高いalpha（32）で強い影響\n",
        "- **creative**: 低いalpha（16）で柔軟な表現\n",
        "- **balanced**: 中間的な値（24）\n",
        "\n",
        "### 6. **マージの実行フロー**\n",
        "\n",
        "1. ベースモデルをロード\n",
        "2. 初期LoRAアダプターを作成\n",
        "3. 追加のアダプターを作成（異なる設定で）\n",
        "4. 重み付きマージを実行\n",
        "5. マージしたアダプターでテキスト生成\n",
        "\n",
        "このコードの優れた点は、**CPU環境でも動作**し、メモリ効率的に複数のLoRAアダプターを組み合わせられることです。実際の用途では、異なるタスクで訓練されたアダプターをマージして、多機能なモデルを作成できます。"
      ],
      "metadata": {
        "id": "LImJCMNItSts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UJrxEPQAtZdE"
      }
    }
  ]
}