{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takedatmh/toyama/blob/main/QLoRA_LLM_FineTuning_for_Japanese_AI_Beginners.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ▶︎ 【QLoRA】ローカル環境で使うための日本語LLMファインチューニング\n",
        "---"
      ],
      "metadata": {
        "id": "rX_fvMxcBlnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 【unsloth編：LLMのファインチューニング入門】<br>\n",
        "このノートブックは、以下の<br><br>\n",
        "・[unslothai/unsloth（Apache-2.0 license）| GitHub ](https://github.com/unslothai/unsloth?tab=readme-ov-file)<br><br>\n",
        "という、大規模言語モデル（LLM：Large Language Model）を<br><br>\n",
        "\n",
        "・「**QLoRA**」(Quantized Low-Rank Adaptation）<br>\n",
        "＊メモリ効率化手法のLoRAに4ビット（NormalFloat - NF4など）以下の量子化を組み合わせた場合には「QLoRA」と呼ぶ<br><br>\n",
        "でファインチューニング（微調整）するプログラムを活用して、ローカル環境で使える対話型生成AIアプリ（ソフトウェア）用に日本語LLMをファインチューニングした「**GGUF形式**」のLLMを保存するためのチュートリアルコードです。<br>\n",
        "一連の情報が、LLMを自分好みにカスタマイズして活用したいと考えている方が、既存のLLMのカスタマイズし、今後の実践利用に向けて検証するために試行錯誤を始めるきっかけになることがありましたら幸いです。\n",
        "<br><br>\n",
        "<br><br>\n",
        "**【Google Colaboratory上のコードの動かし方】**<br><br>\n",
        "まず始めに、Google Colaboratory上のプログラムを実行するには、<br><br>\n",
        "**①Googleアカウントでログイン**<br>\n",
        "**②ドライブにコピーを保存**（QLoRA-LLM-FineTuning-for-Japanese-AI-Beginners.ipynbのノートブックを保存）<br>\n",
        "＊Google Colaboratoryのメニューから「ファイル - ドライブにコピーを保存」\n",
        "<br><br>\n",
        "を実行し、お好きな名前に変更後に以下のコードを実行していきます。<br><br>\n",
        "尚、このノートブックの初期設定は<br><br>\n",
        "**・T4 GPU**<br>\n",
        "＊Google Colaboratory無料枠で使えるGPU<br>\n",
        "＊「**ランタイム - ランタイムのタイプを変更 - ハードウェア アクセラレータ**」で変更できます<br><br>\n",
        "に設定してあります。<br>\n",
        "<br><br>\n",
        "**【チュートリアル動画】**<br><br>\n",
        "2024年3月25日（月）にチュートリアル動画を公開しました。<br><br>\n",
        "[【現代の魔法】QLoRA編：日本語LLMのファインチューニング & ローカル環境アプリで動かす方法  - Fine Tuning LLM & How to Use in Local App by RehabC - デジタルで、遊ぶ。（YouTube）](https://youtu.be/cBY36C1CQeU)<br>\n",
        "視聴時間：53分16秒\n",
        "<br><br>\n",
        "文字情報だけでは、わかりにくい場合などにご活用いただけますと幸いです。\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "8LxhVXHBBv8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**【プログラムのライセンス】**\n",
        "<br>**Apache-2.0 license**\n",
        "\n",
        "©︎ 2024 child programmer<br><br>\n",
        "unsloth is licensed under the Apache-2.0 license, Copyright © unsloth. All Rights Reserved.\n",
        "<br><br>\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "3twJ_IPSFees"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#【ステップ１】ファインチューニング前のLLMで推論 編\n",
        "---\n",
        "後で回答を変化を確認しやすいように、ファインチューニング前のLLMで推論してみましょう。<br>\n",
        "チュートリアルでは<br><br>\n",
        "・「**指示チューニング**」<br>\n",
        "（LLMの基盤モデルに質問と回答を学習させた事前学習済みモデル）<br><br>\n",
        "・「**人間のフィードバックからの強化学習**」<br>\n",
        "（RLHF：Reinforcement Learning from Human Feedback）\n",
        "<br><br>\n",
        "されている<br><br>\n",
        "・[tokyotech-llm/Swallow-7b-instruct-hf | Hugging Face](https://huggingface.co/tokyotech-llm/Swallow-7b-instruct-hf)<br>\n",
        "＊Llama2の日本語能力強化版のSwallowの7B（70億パラメータ）版の指示チューニングLLM<br>\n",
        "＊「東京工業大学情報理工学院の岡崎研究室/横田研究室」「国立研究開発法人産業技術総合研究所」の研究チームにより開発されたLLM\n",
        "<br>\n",
        "注：使用するLLMによって、実行コードを微調整する必要があります。\n",
        "<br><br>\n",
        "を活用して、ファインチューニング前後の変化を比較していきます。\n",
        "<br><br>\n",
        "\n"
      ],
      "metadata": {
        "id": "jVAr8g2xGwbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#【事前準備①】unslothをクローン&依存関係のインストール"
      ],
      "metadata": {
        "id": "LPIQz6BrOJCY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fddKHTUgBNSz",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title 実行コード\n",
        "\n",
        "# 2024年3月16日以降のコード（Google Colaboratoryのアップデートのため）\n",
        "# %%capture\n",
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "# Must install separately since Colab has torch 2.2.1, which breaks packages\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "if major_version >= 8:\n",
        "    # Ampere・Hopperなどの新型GPU (RTX 30xx, RTX 40xx, A100, H100, L40)の場合に以下を実行\n",
        "    !pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "else:\n",
        "    # 旧型のGPU (V100, Tesla T4, RTX 20xx)の場合に以下を実行\n",
        "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "pass\n",
        "\n",
        "\n",
        "# 2024年3月16日時点の依存関係のバージョン\n",
        "# datasets-2.18.0 dill-0.3.8 docstring-parser-0.16 multiprocess-0.70.16 shtab-1.7.1 tyro-0.7.3 unsloth-2024.3 xxhash-3.4.1\n",
        "# accelerate-0.28.0 bitsandbytes-0.43.0 peft-0.9.0 trl-0.7.11 xformers-0.0.25\n",
        "\n",
        "\n",
        "# -2024年3月15日までのコード\n",
        "# # 出力結果を非表示化\n",
        "# %%capture\n",
        "# import torch\n",
        "# major_version, minor_version = torch.cuda.get_device_capability()\n",
        "# if major_version >= 8:\n",
        "#     # Ampere・Hopperなどの新型GPU (RTX 30xx, RTX 40xx, A100, H100, L40)の場合に以下を実行\n",
        "#     !pip install \"unsloth[colab-ampere] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "# else:\n",
        "#     # 旧型のGPU (V100, Tesla T4, RTX 20xx)の場合に以下を実行\n",
        "#     !pip install \"unsloth[colab] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "# pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>"
      ],
      "metadata": {
        "id": "n-b4V2khgLf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#【事前準備②】LLMをダウンロード\n",
        "LLMの一例\n",
        "    # 日本語継続事前学習済みモデル - 基盤モデル（2023年12月公開）\n",
        "    tokyotech-llm/Swallow-7b-hf\n",
        "\n",
        "    #「Swallow-7b-hf」に比べて、より多くの日本語トークンで学習された\n",
        "    # 日本語継続事前学習済みモデル - 基盤モデル（2024年3月公開）\n",
        "    tokyotech-llm/Swallow-7b-plus-hf\n",
        "    \n",
        "    #「Mistral-7B-v0.1」で学習された\n",
        "    # 日本語継続事前学習済みモデル – 基盤モデル（2024年3月公開）\n",
        "    # 7Bモデルだが、Swallow 13Bに迫る日本語性能とのこと\n",
        "    tokyotech-llm/Swallow-MS-7b-v0.1\n",
        "\n",
        "    #「Swallow-7b-hf」の指示チューニングモデル（2023年12月公開）\n",
        "    tokyotech-llm/Swallow-7b-instruct-hf\n",
        "    \n",
        "今回は、ローカル環境でファインチューニング後のLLMを使うために「**instruction**」「**input**」「**output**」からなるAlpaca形式で学習された指示チューニングされた「**Swallow-7b-instruct-hf**」のLLMを利用します。<br><br><br>\n",
        "**【Hugging Faceで現在公開されている日本語LLMをチェック】**<br>\n",
        "[キーワード「japanese」（トレンド順）- Models | Hugging Face](https://huggingface.co/models?sort=trending&search=japanese)<br>\n",
        "**補足説明：**\n",
        "モデル名で散見する「**b**」というのはモデルのパラメータ（重み）が「**billion：10億**」という意味です。<br>\n",
        "＊1.4B：14億パラメータ<br>\n",
        "＊1.7B：17億パラメータ<br>\n",
        "＊3.6B：36億パラメータ<br>\n",
        "＊7B：70億パラメータ<br>\n",
        "＊13B：130億パラメータ<br>\n",
        "＊70B：700億パラメータ<br>\n",
        "<br><br>\n",
        "試した範囲では、東京大学・松尾研究室発のAIカンパニーからリリースされていることで有名なLlama形式で学習されているELYZA（イライザ）の<br><br>\n",
        "[elyza/ELYZA-japanese-Llama-2-7b-fast-instruct | Hugging Face](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast)<br><br>\n",
        "のLLMの場合には、このノートブックでもファインチューニングして推論までできましたが、ローカル環境のアプリで動かそうとするとエラー（アプリがクラッシュ）となるようでした。<br><br>\n",
        "（LLMのダウンロードに3〜16分ほどかかります）"
      ],
      "metadata": {
        "id": "Qws_gDa0On4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 実行コード\n",
        "# LLMダウンロード\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# 最大シーケンス長の指定\n",
        "# 内部でRoPEスケーリングを自動サポート\n",
        "max_seq_length = 2048\n",
        "# データ型dtypeの指定。「None」で自動検出\n",
        "dtype = None\n",
        "# 4-bit量子化。メモリ使用量を減らすために4bit量子化を使用する場合には「True」。「False」でもよい。\n",
        "load_in_4bit = True\n",
        "\n",
        "# 英語版ですが以下の4-bit量子化モデルも指定できるようです\n",
        "# その他の4-bit量子化のLLMは https://huggingface.co/unsloth へ\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
        "    \"unsloth/llama-2-7b-bnb-4bit\",\n",
        "    \"unsloth/llama-2-13b-bnb-4bit\",\n",
        "    \"unsloth/codellama-34b-bnb-4bit\",\n",
        "    \"unsloth/tinyllama-bnb-4bit\",\n",
        "]\n",
        "\n",
        "# LLMの指定など\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    # @markdown  ファインチューニングを実行するLLMを指定します。\n",
        "    model_name = \"tokyotech-llm/Swallow-7b-instruct-hf\" # @param {type:\"string\"}\n",
        "    ,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    #「hf」のモデルで利用。その他の場合にはコメントアウト「#」などで無効化\n",
        "    #「hf：RLHF - Reinforcement Learning from Human Feedback\n",
        "    # 人間のフィードバックからの強化学習\n",
        "    token = \"hf_...\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "Zg0ZUCDYOmxk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 推論①：指示形式"
      ],
      "metadata": {
        "id": "4bm3Y_n_r3OO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 実行コード\n",
        "\n",
        "alpaca_prompt = \"\"\"以下に、あるタスクを説明する指示があり、それに付随する入力が更なる文脈を提供しています。リクエストを適切に完了するための回答を記述してください。\\n\\n\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# 通常の2倍のスピードで推論を実行\n",
        "FastLanguageModel.for_inference(model)\n",
        "# @markdown 推論をするには、以下のプロンプトを入力後にコードを実行します。\n",
        "input_prompt = \"こんにちは。\" # @param {type:\"string\"}\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        # instruction：プロンプトとして活用しています。\n",
        "         input_prompt,\n",
        "        # input：必要に応じて入力します。\n",
        "        \"\",\n",
        "        # output：文章を生成するにはここを空欄にします。\n",
        "        \"\",\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 64) #「max_new_tokens = 」の値で出力のトークン数を指定できます"
      ],
      "metadata": {
        "id": "5nomYwS_P_8U",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 実行コード\n",
        "\n",
        "alpaca_prompt = \"\"\"以下に、あるタスクを説明する指示があり、それに付随する入力が更なる文脈を提供しています。リクエストを適切に完了するための回答を記述してください。\\n\\n\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# 通常の2倍のスピードで推論を実行\n",
        "FastLanguageModel.for_inference(model)\n",
        "# @markdown 推論をするには、以下のプロンプトを入力後にコードを実行します。\n",
        "input_prompt = \"YouTubeのRehabC – デジタルで、遊ぶ。チャンネルについて教えてください。\" # @param {type:\"string\"}\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        # instruction：プロンプトとして活用しています。\n",
        "         input_prompt,\n",
        "        # input：必要に応じて入力します。\n",
        "        \"\",\n",
        "        # output：文章を生成するにはここを空欄にします。\n",
        "        \"\",\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 64) #「max_new_tokens = 」の値で出力のトークン数を指定できます"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-blCg8eYQJi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 推論②：対話型・チャット形式テンプレート"
      ],
      "metadata": {
        "id": "_yOUcnt-scGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 実行コード\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    # サポートしているテンプレート「zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth」\n",
        "    chat_template = \"alpaca\",\n",
        "    # ShareGPTスタイル\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"},\n",
        "    # <|im_end|>を</s>に変換しマッピング\n",
        "    map_eos_token = True,\n",
        ")\n",
        "\n",
        "# 通常の2倍のスピードで推論を実行\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# @markdown 推論をするには、以下のプロンプトを入力後にコードを実行します。\n",
        "input_prompt_chat = \"こんにちは。\" # @param {type:\"string\"}\n",
        "messages = [\n",
        "    {\"from\": \"human\", \"value\": input_prompt_chat},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True) #「max_new_tokens = 」の値で出力のトークン数を指定できます\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "id": "Cu1pYEpXXk7T",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 実行コード\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    # サポートしているテンプレート「zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth」\n",
        "    chat_template = \"alpaca\",\n",
        "    # ShareGPTスタイル\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"},\n",
        "    # <|im_end|>を</s>に変換しマッピング\n",
        "    map_eos_token = True,\n",
        ")\n",
        "\n",
        "# 通常の2倍のスピードで推論を実行\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# @markdown 推論をするには、以下のプロンプトを入力後にコードを実行します。\n",
        "input_prompt_chat = \"YouTubeのRehabC – デジタルで、遊ぶ。チャンネルについて教えてください。\" # @param {type:\"string\"}\n",
        "messages = [\n",
        "    {\"from\": \"human\", \"value\": input_prompt_chat},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-vCikT4qmWuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "7gMF1smtETxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 【ステップ２】LLMのQLoRAファインチューニング 編\n",
        "---\n",
        "それでは、ファインチューニングを始めてみましょう。<br>\n",
        "チュートリアルでは、LLMに特定の知識（今回は、特定のYouTubeチャンネル）<br><br>\n",
        "・[RehabC - デジタルで、遊ぶ。（YouTube）](https://www.youtube.com/channel/UCR04rlF8TZ-xCH2wJvp9WOw)\n",
        "<br><br>\n",
        "を教えてあげます。<br><br>\n"
      ],
      "metadata": {
        "id": "itdtGA7-IDln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#【事前準備①：データセットの準備】\n",
        "（自作カスタムデータセット・Huging Face公開データセット対応）\n",
        "<br>\n",
        "<br>\n",
        "自分で用意したオリジナルデータセットや、Hugging Faceに公開されているデータセットで学習してみましょう。<br>\n",
        "自作データセットを使う場合には、CSV形式ファイルをGoogle Colaboratoryにアップロード後に、以下の「**実行コード - A**」を実行します。<br>\n",
        "また、Hugging Faceの公開データセットを使う場合には、データセット名を指定後に、以下の「**実行コード - B**」を実行します。<br>\n",
        "チュートリアルでは<br><br>\n",
        "**・同じ回答に対する様々なバリエーションの質問など**<br>\n",
        "（学習用19データ）<br><br>\n",
        "を並べた自作データセットを使って、新たな特定の知識などを学習させてみます。\n",
        "<br><br><br>\n",
        "**【データセットの文章例】**<br>\n",
        "**instruction：**<br>\n",
        "YouTubeのRehabC – デジタルで、遊ぶ。チャンネルについて教えてください。<br><br>\n",
        "**input：**<br>\n",
        "（空欄）<br><br>\n",
        "**output：**<br>\n",
        "RehabCチャンネルは、2014年に開設されたデジタルテクノロジー教育系チャンネルです。\n",
        "<br><br>\n",
        "＊　試した範囲の情報では、今回の7BのLLMの場合には、1つの知識を教えるのに、様々なバリエーションに富んだ質問方式にして、同じ回答のデータセットを作成した方が良さそうでした。また、間違える傾向にある質問に関しては、他の質問よりも学習させる数（行）を増やすと正答率が上がる印象を受けています。\n",
        "<br><br><br>\n",
        "\n",
        "**【データセットのテンプレート】**<br>\n",
        "：[ダウンロード - 【QLoRA編】LLMファインチューニング用データセットテンプレート by 子供プログラマー](https://child-programmer.com/download/llm-ft-qlora-tutorial/)\n",
        "<br>\n",
        "＊データセットのテンプレートをエクセル（Windowsの方）やNumbers（Macの方）で編集後に「**dataset**」（dataset.csv）という名前のCSV形式ファイルで書き出してください\n",
        "<br><br>\n",
        "**【チュートリアル用サンプルデータセット】**<br>\n",
        "：[ダウンロード - サンプル： 【QLoRA編】日本語LLMファインチューニング用データセット by 子供プログラマー](https://child-programmer.com/download/llm-ft-qlora-tutorial-sample/)<br>\n",
        "＊ダウンロード時点のファイル名は「**sample_dataset**」（sample_dataset.csv）ですが、利用する際には「**dataset**」（dataset.csv）という名前に変更してご利用ください\n",
        "<br><br>\n",
        "\n",
        "**【参考情報】**<br>\n",
        "推奨されるデータセットの量：1000～50000<br>\n",
        "[Documentation - FAQs：How much data is generally required to fine-tune a model? | H2O.AI](https://h2oai.github.io/h2o-llmstudio/faqs#how-much-data-is-generally-required-to-fine-tune-a-model)<br>\n",
        "（大規模言語モデルをファインチューニングするには一般的にどのくらいのデータ量が必要ですか?）<br><br>"
      ],
      "metadata": {
        "id": "SrNGQe4GX1eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 実行コード - A（自作のカスタムデータセット版）\n",
        "alpaca_prompt = \"\"\"以下に、あるタスクを説明する指示があり、それに付随する入力が更なる文脈を提供しています。リクエストを適切に完了するための回答を記述してください。\\n\\n\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        #「EOS_TOKEN」を追加\n",
        "        # 生成が永遠に続いてしまうため\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "\n",
        "# データセットを処理するために「load_dataset」をインポート\n",
        "from datasets import load_dataset\n",
        "\n",
        "# データセットの設定（自作データセット）の設定 - はじめ\n",
        "# @markdown  データセットのファイルのパス（CSV形式のデータセット）を指定します。\n",
        "dataset_files = \"/content/dataset.csv\" # @param {type:\"string\"}\n",
        "dataset = load_dataset(\"csv\", data_files = dataset_files, split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
        "# データセットの設定（自作データセット）の設定 - おわり\n",
        "\n"
      ],
      "metadata": {
        "id": "tZH6a-T8YoEV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>"
      ],
      "metadata": {
        "id": "1uUBruRYvuYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 実行コード - B（Huging Face公開データセット版）\n",
        "alpaca_prompt = \"\"\"以下に、あるタスクを説明する指示があり、それに付随する入力が更なる文脈を提供しています。リクエストを適切に完了するための回答を記述してください。\\n\\n\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        #「EOS_TOKEN」を追加\n",
        "        # 生成が永遠に続いてしまうため\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "\n",
        "# データセットを処理するために「load_dataset」をインポート\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "#↓Hugging Faceの既存のデータセットを使いたい場合には以下のコードを参考に\n",
        "# コードをカスタマイズしてみてください\n",
        "\n",
        "# データセットの設定（Hugging Faceの公開データセット） - はじめ\n",
        "#「fujiki/japanese_alpaca_data」（データセットの量：52,002）を使う場合の例\n",
        "#（注：無料枠のGPU　- T4 GPUでは学習回数は1回〜数回程度かもしれません）\n",
        "# データセットは最終的に 「instruction」「input」「output」の列になるように\n",
        "# 前処理をしてください\n",
        "\n",
        "# @markdown  Hugging Faceの該当データセットの名前を指定します。\n",
        "llm_hf_dataset = \"fujiki/japanese_alpaca_data\" # @param {type:\"string\"}\n",
        "dataset = load_dataset(llm_hf_dataset, split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
        "\n",
        "# データセットの設定（Hugging Faceの公開データセット） - おわり\n",
        "\n",
        "\n",
        "\n",
        "# 【おまけのコード】\n",
        "# データセットの設定（Hugging Faceの公開データセット） - はじめ\n",
        "#「izumi-lab/llm-japanese-dataset」（データセットの量：9,074,340）を使う場合の例\n",
        "#（注：この規模だと無料枠のGPU　- T4 GPUではきついかもしれません）\n",
        "# データセットは最終的に 「instruction」「input」「output」の列になるように\n",
        "#  前処理をしてください\n",
        "# Hugging Faceの既存のデータセットを使いたい場合には以下のコードを有効化します\n",
        "\n",
        "# dataset = load_dataset(\"izumi-lab/llm-japanese-dataset\", split = \"train\")\n",
        "# dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
        "\n",
        "# データセットの設定（Hugging Faceの公開データセット） - おわり"
      ],
      "metadata": {
        "cellView": "form",
        "id": "R2gXrnANzZ5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**【データセットの例】**<br>\n",
        "・[fujiki/japanese_alpaca_data（cc-by-nc-sa-4.0）| Hugging Face](https://huggingface.co/datasets/fujiki/japanese_alpaca_data)<br>\n",
        "　データセット量：52,002<br><br>\n",
        "・[izumi-lab/llm-japanese-dataset（cc-by-sa-4.0）| Hugging Face](https://huggingface.co/datasets/izumi-lab/llm-japanese-dataset)<br>\n",
        "　データセット量：9,074,340<br><br>\n",
        "データセットは最終的に 「**instruction**」「**input**」「**output**」の列になるように前処理をしてください。<br><br>\n",
        "\n",
        "**【Hugging Faceで現在公開されている日本語データセットをチェック】**<br>\n",
        "[キーワード「japanese」（トレンド順）- Datasets | Hugging Face](https://huggingface.co/datasets?sort=trending&search=japanese)\n",
        "<br><br><br>\n"
      ],
      "metadata": {
        "id": "ndADUaAoeUbt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#【事前準備②：ファインチューニングのパラメータの指定】\n",
        "「**実行コード①**」（LoRAの設定）は、1回のみ実行可能です。2回目の実行は反映されません。<br>\n",
        "「**実行コード②**」（その他のパラメータの設定）は、何度でも実行可能です。<br>\n",
        "各種パラメータの詳細をコード内に記載しておきました。<br>\n",
        "必要に応じて「**コードの表示**」をクリックして、設定を変更してみてください。"
      ],
      "metadata": {
        "id": "FCRmCBqThd8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 実行コード①（LoRAのパラメータの設定）\n",
        "\n",
        "# LoRA（Low-Rank Adaptation）の設定\n",
        "# 少ない学習パラメータで調整(全ての重みではなく、近似した小規模の行列を調整）\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    # LLMモデルの指定\n",
        "    model,\n",
        "    # LoRA Rディメンション（次元）の指定\n",
        "    # 更新行列のランクを表すパラメータ\n",
        "    # 一般的にrが小さいと、より短時間で計算量が少なくなる。大きいほど、更新行列は元の重みに近くなるが計算量が増える\n",
        "    # ここの値を大きくするとファインチューニングを行うパラメータの割合を増やすことができます\n",
        "    r = 16, # デフォルト 16 # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    # LoRAを適用するモジュールの指定\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    # LoRAアルファの指定\n",
        "    # LoRAのスケーリングに使用されるパラメータ。\n",
        "    # 更新行列の大きさを制限し、LoRAの過学習などを抑制\n",
        "    lora_alpha = 16,\n",
        "    # LoRAのドロップアウト率の指定\n",
        "    # 更新行列の一部を無効化し、LoRAの過学習などを抑制\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # メモリの最適化 - 勾配チェックポインティング\n",
        "    # 「per_device_train_batch_size = 1」でもGPUメモリがオーバーしてしまう時に、メモリを最適化する方法\n",
        "    # 使用する場合には「use_gradient_checkpointing = True」に設定\n",
        "    use_gradient_checkpointing = True,\n",
        "    # ランダムシード値の指定\n",
        "    random_state = 3407,\n",
        "    # Rank-Stabilized LoRAの設定\n",
        "    # 有効化したい場合には「use_rslora = True」に設定\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    # LoRA-Fine-Tuning-Aware Quantizationの設定\n",
        "    # 有効化したい場合には「loftq_config = loftq_config」に設定\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "nWZfFJIyBCTs",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 実行コード②（その他のパラメータの設定）\n",
        "# ファインチューニング用に各種インポート\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# ファインチューニングのその他のパラメータの設定\n",
        "trainer = SFTTrainer(\n",
        "    # LLMモデルの指定\n",
        "    model = model,\n",
        "    # トークナイザの指定\n",
        "    tokenizer = tokenizer,\n",
        "    # 学習用のデータセットを指定\n",
        "    train_dataset = dataset,\n",
        "    # 学習用のデータセットのテキストフィールドの指定\n",
        "    dataset_text_field = \"text\",\n",
        "    # 最大シーケンス長の指定\n",
        "    max_seq_length = max_seq_length,\n",
        "    # データのトークン化に使用するワーカー数の指定\n",
        "    dataset_num_proc = 2,\n",
        "    # シークエンスパッキングの設定\n",
        "    # 短いシークエンスであれば、トレーニングを5倍速くすることができる\n",
        "    # 「True」で有効化\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        # バッチサイズの指定\n",
        "        per_device_train_batch_size = 2,\n",
        "        # メモリの最適化：勾配累積のステップ数（エポック数）の指定\n",
        "        # バッチ全体の勾配を一度に計算せずに、小さなバッチサイズで計算したものを集約しでバッチサイズを増やす\n",
        "        gradient_accumulation_steps = 4,\n",
        "        # 学習率をウォームアップするステップ数（エポック数）を指定\n",
        "        # 「warmup_epochs=0」で0から増加\n",
        "        warmup_steps = 5,\n",
        "        # ファインチューニングの学習回数（ステップ数・エポック数）を指定\n",
        "        # @markdown ファインチューニングの学習回数を指定します。\n",
        "        max_steps = 30 # @param {type:\"integer\"}\n",
        "        ,\n",
        "        # 学習率の指定\n",
        "        # 学習の際に重みを更新する際に使用する割合\n",
        "        # 過学習（過剰適合・オーバーフィッティング）と学習不足（未学習・アンダーフィット）のバランスをとるための設定\n",
        "        learning_rate = 2e-4,\n",
        "        # FP16（16ビット浮動小数点）の使用有無を指定。「not」を削除すると有効化\n",
        "        # LLMの重み（パラメータ）を16ビット精度 (FP16) でロード「float16 量子化」\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        # BFP16（Brain Float Point 16）の使用有無を指定。「= not torch.cuda.is_bf16_supported()」で無効化\n",
        "        # LLMの重み（パラメータ）を16ビット精度 (BFP16) でロード「bfloat16 量子化」\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        # 学習ログを記録する頻度を指定\n",
        "        logging_steps = 1,\n",
        "        # 最適化アルゴリズムにAdamW 8ビットを利用\n",
        "        optim = \"adamw_8bit\",\n",
        "        # 重み減衰の指定\n",
        "        # 値を0以外（例：0.01）にすると、L2正規化が働き過学習の抑制効果\n",
        "        weight_decay = 0.01,\n",
        "        # 学習率のスケジュールの定義\n",
        "        # 「linear」（線形の学習率を適用）\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        # ランダムシード値の指定\n",
        "        seed = 3407,\n",
        "        # 結果の出力先\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")\n"
      ],
      "metadata": {
        "id": "q-yFIbIUKbAO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br><br>"
      ],
      "metadata": {
        "id": "ozJIiXR-wQDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ファインチューニングの実行"
      ],
      "metadata": {
        "id": "eaLCEf0p45Ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 実行コード\n",
        "# メモリのステータスの表示設定\n",
        "# （0番目の）GPUの情報を取得\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "# 予約メモリの最大GPUメモリ確保量を小数点以下3桁でGB表示\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "# 使用するGPUのGPUメモリ容量を小数点以下3桁でGB表示\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "# GPU名とメモリ容量を出力\n",
        "print(f\"GPU = {gpu_stats.name}. 最大メモリ = {max_memory} GB.\")\n",
        "# 確保しているGPUメモリの量を出力\n",
        "print(f\"予約メモリ = {start_gpu_memory} GBを確保\")\n",
        "\n",
        "\n",
        "# ファインチューニングの実行\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "\n",
        "# ファインチューニングに要した時間・GPUのの統計情報の表示設定\n",
        "# 予約メモリの最大GPUメモリ確保量を小数点以下3桁でGB表示\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "# ファインチューニングに要した最大GPUメモリを小数点以下3桁でGB表示\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "# 使用したGPUのメモリ容量に対するファインチューニングで予約メモリの割合を計算し小数点以下3桁でGB表示\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "# 使用したGPUのメモリ容量に対するファインチューニングに要したメモリ消費量の割合を計算し小数点以下3桁でGB表示\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "# ファインチューニングに要した時間を出力\n",
        "print(f\"学習時間 = {trainer_stats.metrics['train_runtime']} （秒）.\")\n",
        "print(f\"学習時間 = {round(trainer_stats.metrics['train_runtime']/60, 2)} （分）\")\n",
        "print(f\"予約メモリの最大GPUメモリ = {used_memory} GB.\")\n",
        "print(f\"学習に要した予約メモリの最大GPUメモリ = {used_memory_for_lora} GB.\")\n",
        "print(f\"GPUのメモリ使用率（予約メモリ/GPU容量） = {used_percentage} %.\")\n",
        "print(f\"GPUのメモリ使用率（学習に要したメモリ消費量/GPU容量） = {lora_percentage} %.\")"
      ],
      "metadata": {
        "id": "3Gf52IT444d6",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br><br>"
      ],
      "metadata": {
        "id": "4HxMFagAwTfO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ファインチューニングしたLLMで推論\n",
        "それでは、先程ファインチューニングした学習モデルを使って推論をしてみましょう。"
      ],
      "metadata": {
        "id": "Z3rRtd1z24kV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 推論①：指示形式"
      ],
      "metadata": {
        "id": "XjClsYcTfk8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 実行コード\n",
        "\n",
        "alpaca_prompt = \"\"\"以下に、あるタスクを説明する指示があり、それに付随する入力が更なる文脈を提供しています。リクエストを適切に完了するための回答を記述してください。\\n\\n\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# 通常の2倍のスピードで推論を実行\n",
        "FastLanguageModel.for_inference(model)\n",
        "# @markdown 推論をするには、以下のプロンプトを入力後にコードを実行します。\n",
        "input_prompt = \"こんにちは。\" # @param {type:\"string\"}\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        # instruction：プロンプトとして活用しています。\n",
        "         input_prompt,\n",
        "        # input：必要に応じて入力します。\n",
        "        \"\",\n",
        "        # output：文章を生成するにはここを空欄にします。\n",
        "        \"\",\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 64) #「max_new_tokens = 」の値で出力のトークン数を指定できます"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TGbYqUWC_V-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 実行コード\n",
        "\n",
        "alpaca_prompt = \"\"\"以下に、あるタスクを説明する指示があり、それに付随する入力が更なる文脈を提供しています。リクエストを適切に完了するための回答を記述してください。\\n\\n\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "# 通常の2倍のスピードで推論を実行\n",
        "FastLanguageModel.for_inference(model)\n",
        "# @markdown 推論をするには、以下のプロンプトを入力後にコードを実行します。\n",
        "input_prompt = \"YouTubeのRehabC – デジタルで、遊ぶ。チャンネルについて教えてください。\" # @param {type:\"string\"}\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        # instruction：プロンプトとして活用しています。\n",
        "         input_prompt,\n",
        "        # input：必要に応じて入力します。\n",
        "        \"\",\n",
        "        # output：文章を生成するにはここを空欄にします。\n",
        "        \"\",\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 64) #「max_new_tokens = 」の値で出力のトークン数を指定できます"
      ],
      "metadata": {
        "id": "J2lBwkKWcjhB",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>"
      ],
      "metadata": {
        "id": "U9A9oHvwwW_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 推論②：対話型・チャット形式テンプレート"
      ],
      "metadata": {
        "id": "aHdiyvrwfuDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 実行コード\n",
        "\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    # サポートしているテンプレート「zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth」\n",
        "    chat_template = \"alpaca\",\n",
        "    # ShareGPTスタイル\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"},\n",
        "    # <|im_end|>を</s>に変換しマッピング\n",
        "    map_eos_token = True,\n",
        ")\n",
        "\n",
        "# 通常の2倍のスピードで推論を実行\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# @markdown 推論をするには、以下のプロンプトを入力後にコードを実行します。\n",
        "input_prompt_chat = \"こんにちは。\" # @param {type:\"string\"}\n",
        "messages = [\n",
        "    {\"from\": \"human\", \"value\": input_prompt_chat},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True) #「max_new_tokens = 」の値で出力のトークン数を指定できます\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FYHDu_-7_LNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 実行コード\n",
        "\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    # サポートしているテンプレート「zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth」\n",
        "    chat_template = \"alpaca\",\n",
        "    # ShareGPTスタイル\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"},\n",
        "    # <|im_end|>を</s>に変換しマッピング\n",
        "    map_eos_token = True,\n",
        ")\n",
        "\n",
        "# 通常の2倍のスピードで推論を実行\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# @markdown 推論をするには、以下のプロンプトを入力後にコードを実行します。\n",
        "input_prompt_chat = \"YouTubeのRehabC – デジタルで、遊ぶ。チャンネルについて教えてください。\" # @param {type:\"string\"}\n",
        "messages = [\n",
        "    {\"from\": \"human\", \"value\": input_prompt_chat},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True) #「max_new_tokens = 」の値で出力のトークン数を指定できます\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "id": "IHEoHsEgRjp2",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br><br>"
      ],
      "metadata": {
        "id": "Bzc3UMlMf6ag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#【おまけコード】ファインチューニングしたLLMをGGUF形式にする前に保存したい場合\n",
        "必要な箇所のコードの「**False**」の記載を「**True**」に変更後にコードを実行します。<br>\n",
        "「**4-bit量子化としてLLMの結果を保存**」したい場合には<br>\n",
        "「**if True: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit_forced\",)**」<br>\n",
        "にコードを変更後にコードを実行します。<br><br>\n",
        "不必要な場合には、ここのコードを飛ばしてください。\n"
      ],
      "metadata": {
        "id": "IHzXVDGVKnLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 実行コード\n",
        "\n",
        "# 16-bit量子化としてLLMの結果を保存\n",
        "# ただ、この後実行するGGUF化の際に、処理の経過として16-bit量子化のLLMが出力されます\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "# 16bit量子化としてLLMの結果を保存したものをHugging Faceにプッシュ\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# 4-bit量子化としてLLMの結果を保存\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit_forced\",)\n",
        "# 4-bit量子化としてLLMの結果を保存したものをHugging Faceにプッシュ\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit_forced\", token = \"\")\n",
        "\n",
        "# LoRAアダプターのみ保存\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "# LoRAアダプターのみ保存保存したものをHugging Faceにプッシュ\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ],
      "metadata": {
        "id": "D0MOiaqnKl5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br><br>"
      ],
      "metadata": {
        "id": "R0GatPPSgBVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ファインチューニングしたLLMを「量子化」+「GGUF形式」に変換\n",
        "ローカル環境のアプリで使うために、QLoRAしたLLMをGGUF形式に変換して保存します。<br>\n",
        "変換の過程で<br><br>\n",
        "**・float 16のLLMが「model」フォルダ内に出力**<br>\n",
        "（一例：12GB台）<br>\n",
        "\n",
        "・**float 16の「GGUF形式」に変換された「model-unsloth.F16.gguf」ファイルが出力**<br>\n",
        "（一例：12GB台）<br><br>\n",
        "されます。<br>\n",
        "その後、最終的に指定した量子化手法の形式のファイルが出力されます。<br>\n",
        "一例：「**q2_k**」の量子化手法の場合、最終的に「**model-unsloth.Q2_K.gguf**」（2GB台）ファイルが出力されます。<br>\n",
        "一例：「**q4_k_m**」の量子化手法の場合、最終的に「**model-unsloth.Q4_K_M.gguf**」（3〜4GB台）ファイルが出力されます。<br><br>\n",
        "（一連の処理を実行するのに20分前後かかります）"
      ],
      "metadata": {
        "id": "Ntd3UXRt9NK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 実行コード\n",
        "\n",
        "# 「q4_k_m」（4-bit量子化）でGGUF形式に保存したい場合に利用\n",
        "if True: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "\n",
        "# 以下のコードは、Hagging Faceにプッシュしたい場合に利用\n",
        "# 「False」を「True」にすると有効化されます\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")"
      ],
      "metadata": {
        "id": "epdnv73-vqa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**【参考情報：対応している量子化の手法の一例】**<br>（float 32/bfloat 32の際に26GBとなる7Bモデルの例・型の区分は容量で区分した大まかな分類）<br><br>\n",
        "「**quantization_method =**」のコード内に以下の量子化の手法を記入し、上記のコードを実行。<br>\n",
        "「**q2_k**」を使う場合には半角英数で「**quantization_method = \"q2_k\"**」と記入します。<br><br>\n",
        "**・q2_k**（2-bit量子化）<br>\n",
        "：最小型（一例：2.63GB）。<br>\n",
        "attention.vwとfeed_forward.w2のテンソルにはQ4_Kを使用し、その他のテンソルにはQ2_Kを使用する。<br>＊注「K」：k-quantメソッドという量子化モデル\n",
        "<br>\n",
        "**・q3_k_s**（3-bit量子化）<br>\n",
        "：超小型（一例：2.75GB）。<br>\n",
        "すべてのテンソルにQ3_Kを使用する。<br>\n",
        "**・q3_k_m**（3-bit量子化）<br>\n",
        " ：超小型（一例：3.07GB）。<br>\n",
        " attention.wv、attention.wo、feed_forward.w2のテンソルにはQ4_Kを使用し、それ以外のテンソルにはQ3_Kを使用する。<br>\n",
        "**・q3_k_l**（3-bit量子化）<br>\n",
        "：小型（一例：3.35GB）。<br>\n",
        "attention.wv、attention.wo、feed_forward.w2のテンソルにはQ5_Kを使用し、それ以外のテンソルにはQ3_Kを使用する。<br>\n",
        "**・q4_0**（4-bit量子化）<br>\n",
        "：小型（一例：3.56G）。<br>\n",
        "オリジナル版の4-bit量子化を使用する。<br>\n",
        "**・q4_k_s**（4-bit量子化）<br>\n",
        "：小型（一例：3.59GB）。<br>\n",
        "すべてのテンソルにQ4_Kを使用する。<br>\n",
        "**・q4_k_m【推奨】**（4-bit量子化）<br>\n",
        "：中型 (一例：3.80GB)<br>\n",
        "attention.wvとfeed_forward.w2のテンソルの半分にQ6_Kを使用する。それ以外はQ4_Kを使用する。<br>\n",
        "**・q4_1**（4-bit量子化）<br>\n",
        "：中型（一例：3.90GB）。<br>\n",
        "q4_0よりは精度が高いが、q5_0ほどではない。しかし、q5モデルよりも推論が速い。<br>\n",
        "**・q5_0**（5-bit量子化）<br>\n",
        "：中型（一例：4.33GB）。<br>\n",
        "高精度だが、リソースの使用量が多いため、低速の推論となる。<br>\n",
        "**・q5_k_s**（5-bit量子化）<br>\n",
        "：大型（一例：4.33GB）。<br>\n",
        "すべてのテンソルにQ5_Kを使用する。<br>\n",
        "**・q5_k_m【推奨】**（5-bit量子化）<br>\n",
        "：大型（一例：4.45GB）。<br>\n",
        "attention.wvとfeed_forward.w2のテンソルの半分にQ6_Kを使用する。それ以外はQ5_Kを使用する。<br>\n",
        "**・q5_1**（5-bit量子化）<br>\n",
        "：大型（一例：4.7GB）。<br>\n",
        "q5_0よりも、さらに高い精度だが、リソースの使用量が多いため、さらに低速の推論となる。<br>\n",
        "**・q6_k**（6-bit量子化）<br>\n",
        "：超大型（一例：5.15GB）。<br>\n",
        "すべてのテンソルにQ8_Kを使用する。<br>\n",
        "**・q8_0**（8-bit量子化）<br>\n",
        "：超大型（一例：6.7GB）。<br>\n",
        "float16とほとんど区別がつかないレベルだが、リソースの使用量が多いため、低速の推論となる。ほとんどのユーザーにはお勧めできない。<br>\n",
        "<br>\n",
        "**【参考】**<br>\n",
        "・[quantize.cpp - ggerganov/llama.cpp | GitHub](https://github.com/ggerganov/llama.cpp/blob/master/examples/quantize/quantize.cpp)<br>\n",
        "（各量子化手法の容量）\n",
        "<br>\n",
        "・[save.py - unslothai/unsloth](https://github.com/unslothai/unsloth/blob/main/unsloth/save.py)<br>\n",
        "（量子化のバリエーションと説明）<br><br><br>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "buhTGIGW2vhH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google DriveのマウントとGGUF形式ファイルのダウンロード\n",
        " [Google Drive](https://www.google.com/intl/ja_ALL/drive/)  をマウントし、「**GGUF形式ファイル**」（例：model-unsloth.Q4_K_M.gguf）を「**MyDrive**」に移動後に、お使いのパソコンのローカル環境にダウンロードします。<br><br><br>"
      ],
      "metadata": {
        "id": "mNjA8ySAIW3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 実行コード\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1hqymhnxIQyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br><br>"
      ],
      "metadata": {
        "id": "BcDP21Rmo_To"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#【おまけコード】Janで使う前に量子化したGGUFのLLMで検証\n",
        "\n",
        "必要に応じて、以下のチュートリアルコード<br><br>\n",
        "[llama-cpp-python-for-Japanese-AI-Beginners.ipynb（The MIT License）| Google Colaboratory](https://colab.research.google.com/drive/1yZ1KCToAmKTSgWjjVpLzF48gwoHOq2yx?usp=sharing)<br><br>\n",
        "を参考に、このノートブックに「**手順①**」「**手順③**」のコードをコピー&ペーストしてコードを実行し、量子化したGGUF形式のLLMの回答の精度を検証してみてください。<br><br><br>\n"
      ],
      "metadata": {
        "id": "Wzir-NNtl9_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br><br>"
      ],
      "metadata": {
        "id": "Ud2MdoMTFUWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 【ステップ３】ローカル環境でファインチューニングしたLLMを実行 編\n",
        "---\n",
        "それでは、ローカル環境のアプリで量子化＋GGUF化したLLMを実行してみましょう。<br><br>\n",
        "まずは、以下のリンク先<br><br>\n",
        "**ChatGPT代替AIアプリをダウンロード：**<br>\n",
        "[Jan：Open-source ChatGPT alternative（AGPLv3 License.）| Jan AI  ](https://jan.ai/)<br>\n",
        "（対応OS：Windows・M1/M2/M3 Mac・Intel Mac・Linux AppImage/deb）<br><br>\n",
        "のページで、現在お使い中のOSのバージョンのJanのアプリをダウンロードします。<br>\n",
        "Janのアプリで、今回ファインチューニングしたLLMを使えるようにする方法は、以下の記事<br><br>\n",
        "\n",
        "**Jan公式の解説記事：**<br>\n",
        "[Import Models Manually | Jan AI](https://jan.ai/guides/using-models/import-manually/)<br>\n",
        "（LLMを手動でアプリ内にインポートする方法）<br>\n",
        "2024年3月23日時点では編集中になっていました...\n",
        "<br><br>\n",
        "を参照ください。<br>\n",
        "インポートの手順例やアプリの使い方は、チュートリアル動画で解説予定です。<br><br>\n",
        "**【追記：2024年3月23日】**<br>\n",
        "アプリがアップデートされ簡単にGGUF形式のモデルをインポートできるようになりました。<br>\n",
        "「**Settings**」の「**My Models**」の表示画面の右上にある「**Import Model**」をクリック後に表示されるウインドウに、ドラッグ&ドロップするとインポートできます。<br>\n",
        "チュートリアル動画では「**Move model binary file**」を選択し、Janのアプリ内にモデルのファイルを複製させて使う方法を実行しています。\n",
        "<br><br><br>\n",
        "また、このノートブックで「**ELYZA-japanese-Llama-2-7b**」をファインチューニングしたものをJanのアプリで使えなかったため、Hugging Faceに公開してくださっている<br><br>\n",
        "\n",
        "・[elyza/ELYZA-japanese-Llama-2-7b-fast-instruct | Hugging Face](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-fast)<br><br>\n",
        "\n",
        "の量子化版LLM<br><br>\n",
        "・[mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf | Hugging Face](https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf)<br><br>\n",
        "のGGUF形式ファイルをJanで使う方法も解説予定です。<br><br>\n",
        "**【追記：2024年3月25日】**<br>\n",
        "チュートリアル動画を公開しました。<br>\n",
        "Janのアプリにファインチューニング後にGGUF化したLLMをインポートする方法や、ELYZAをJanのアプリで使う方法は、動画を参照いただけますと幸いです。\n"
      ],
      "metadata": {
        "id": "hjVij8iiI-4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 【コピー & ペースト用素材】\n",
        "**①Swallow用**<br>\n",
        "**【Assistant】Instructions**<br><br>\n",
        "以下に、あるタスクを説明する指示があり、それに付随する入力が更なる文脈を提供しています。リクエストを適切に完了するための回答を記述してください。\n",
        "<br><br>\n",
        "**②ELYZA用**<br>\n",
        "**【Assistant】Instructions**<br><br>\n",
        "あなたは誠実で優秀な日本人のアシスタントです。\n",
        "<br><br>\n",
        "**【Model Parameters】Prompt template**"
      ],
      "metadata": {
        "id": "27K3WA1nd4uF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[INST] <<SYS>>\\n{system_message}<</SYS>>\\n{prompt}[/INST]"
      ],
      "metadata": {
        "id": "spg1qQmnfPb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>**【チュートリアル記事】**<br>\n",
        "<br>\n",
        "：[【QLoRA編】日本語LLMのファインチューニング & 低スペックのローカル環境のアプリで動かす by 子供プログラマー](https://child-programmer.com/llm-ft-qlora-tutorial/)<br>\n"
      ],
      "metadata": {
        "id": "3LUVOu29FAsf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>【公開日】2024年3月17日<br>\n",
        "【最終更新】2024年3月31日<br>\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "mPJo-jm5FOEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n",
        "[日本人のための人工知能プログラマー入門講座（機械学習）by 子供プログラマー](https://child-programmer.com/ai/)"
      ],
      "metadata": {
        "id": "RYVYhq6FFYdb"
      }
    }
  ]
}